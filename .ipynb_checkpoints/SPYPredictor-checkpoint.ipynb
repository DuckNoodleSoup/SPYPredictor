{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>SPYPredictor</h2>\n",
    "Final Project Shuzo Katayama, 11 Deecember 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "15 November 2020: \n",
    "\n",
    "Progress for the Prototype:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data:\n",
    "\n",
    "SPY Prices: https://finance.yahoo.com/quote/SPY/history?period1=728265600&period2=1604966400&interval=1mo&filter=history&frequency=1mo&includeAdjustedClose=true \\\n",
    "GDP: https://fred.stlouisfed.org/series/GDP \\\n",
    "Unemployment: https://fred.stlouisfed.org/series/UNRATE \\\n",
    "Federal Funds Rate: https://fred.stlouisfed.org/series/FEDFUNDS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the data:\\\n",
    "All the data will come initially in a DataFrame with the name of the data followed by 'df'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1993-01-29</td>\n",
       "      <td>43.968750</td>\n",
       "      <td>43.968750</td>\n",
       "      <td>43.750000</td>\n",
       "      <td>43.937500</td>\n",
       "      <td>26.079659</td>\n",
       "      <td>1003200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1993-02-01</td>\n",
       "      <td>43.968750</td>\n",
       "      <td>44.250000</td>\n",
       "      <td>43.968750</td>\n",
       "      <td>44.250000</td>\n",
       "      <td>26.265144</td>\n",
       "      <td>480500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1993-02-02</td>\n",
       "      <td>44.218750</td>\n",
       "      <td>44.375000</td>\n",
       "      <td>44.125000</td>\n",
       "      <td>44.343750</td>\n",
       "      <td>26.320782</td>\n",
       "      <td>201300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1993-02-03</td>\n",
       "      <td>44.406250</td>\n",
       "      <td>44.843750</td>\n",
       "      <td>44.375000</td>\n",
       "      <td>44.812500</td>\n",
       "      <td>26.599014</td>\n",
       "      <td>529400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1993-02-04</td>\n",
       "      <td>44.968750</td>\n",
       "      <td>45.093750</td>\n",
       "      <td>44.468750</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>26.710312</td>\n",
       "      <td>531500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6992</th>\n",
       "      <td>2020-11-03</td>\n",
       "      <td>333.690002</td>\n",
       "      <td>338.250000</td>\n",
       "      <td>330.290009</td>\n",
       "      <td>336.029999</td>\n",
       "      <td>336.029999</td>\n",
       "      <td>93294200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6993</th>\n",
       "      <td>2020-11-04</td>\n",
       "      <td>340.859985</td>\n",
       "      <td>347.940002</td>\n",
       "      <td>339.589996</td>\n",
       "      <td>343.540009</td>\n",
       "      <td>343.540009</td>\n",
       "      <td>126959700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6994</th>\n",
       "      <td>2020-11-05</td>\n",
       "      <td>349.239990</td>\n",
       "      <td>352.190002</td>\n",
       "      <td>348.859985</td>\n",
       "      <td>350.239990</td>\n",
       "      <td>350.239990</td>\n",
       "      <td>82039700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6995</th>\n",
       "      <td>2020-11-06</td>\n",
       "      <td>349.929993</td>\n",
       "      <td>351.510010</td>\n",
       "      <td>347.649994</td>\n",
       "      <td>350.160004</td>\n",
       "      <td>350.160004</td>\n",
       "      <td>74973000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6996</th>\n",
       "      <td>2020-11-09</td>\n",
       "      <td>363.970001</td>\n",
       "      <td>364.380005</td>\n",
       "      <td>354.059998</td>\n",
       "      <td>354.559998</td>\n",
       "      <td>354.559998</td>\n",
       "      <td>171760300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6997 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date        Open        High         Low       Close   Adj Close  \\\n",
       "0     1993-01-29   43.968750   43.968750   43.750000   43.937500   26.079659   \n",
       "1     1993-02-01   43.968750   44.250000   43.968750   44.250000   26.265144   \n",
       "2     1993-02-02   44.218750   44.375000   44.125000   44.343750   26.320782   \n",
       "3     1993-02-03   44.406250   44.843750   44.375000   44.812500   26.599014   \n",
       "4     1993-02-04   44.968750   45.093750   44.468750   45.000000   26.710312   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "6992  2020-11-03  333.690002  338.250000  330.290009  336.029999  336.029999   \n",
       "6993  2020-11-04  340.859985  347.940002  339.589996  343.540009  343.540009   \n",
       "6994  2020-11-05  349.239990  352.190002  348.859985  350.239990  350.239990   \n",
       "6995  2020-11-06  349.929993  351.510010  347.649994  350.160004  350.160004   \n",
       "6996  2020-11-09  363.970001  364.380005  354.059998  354.559998  354.559998   \n",
       "\n",
       "         Volume  \n",
       "0       1003200  \n",
       "1        480500  \n",
       "2        201300  \n",
       "3        529400  \n",
       "4        531500  \n",
       "...         ...  \n",
       "6992   93294200  \n",
       "6993  126959700  \n",
       "6994   82039700  \n",
       "6995   74973000  \n",
       "6996  171760300  \n",
       "\n",
       "[6997 rows x 7 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Daily prices of SPY\n",
    "SPYdf = pd.read_csv('SPYDaily.csv')\n",
    "SPYdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "GDPdf = pd.read_csv('GDPQuarterly.csv')\n",
    "Unemploymentdf = pd.read_csv('UnemploymentMonthly.csv')\n",
    "Unemploymentdf['UNRATE'].astype('float')\n",
    "FedFundspd = pd.read_csv('Fedfunds.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting Data into X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training targets are the closing prices of SPY\n",
    "y = SPYdf.to_numpy()\n",
    "y = y[:, 4]\n",
    "      \n",
    "# Initialise Xdf as initially having 4 columns and the number of rows in SPYdf\n",
    "c = 4\n",
    "r = len(SPYdf)\n",
    "\n",
    "a = SPYdf.to_numpy()\n",
    "X = np.empty((r,c))\n",
    "\n",
    "# Keep a separate array of the dates\n",
    "Xdate = SPYdf.to_numpy()\n",
    "Xdate = Xdate[:,0]\n",
    "\n",
    "# Copy into column 0 in X, the opening price from a\n",
    "counter = 0\n",
    "for item in a:\n",
    "    X[counter, 0] = item[1]\n",
    "    counter = counter+1\n",
    "    \n",
    "# Copy in to column 1, the GDP\n",
    "# GDP is calculated quarterly, and SPY is daily; writing GDP value every day in the quarter in X\n",
    "g = GDPdf.to_numpy()\n",
    "counter = r-1\n",
    "gdpcounter = (len(g)-1)\n",
    "for item in X:\n",
    "    if g[gdpcounter,0] == Xdate[counter]:\n",
    "        gdpcounter = gdpcounter - 1\n",
    "    \n",
    "    item[1] = g[gdpcounter, 1]\n",
    "    counter = counter - 1\n",
    "    \n",
    "# Copy into column 2, the Unemployment\n",
    "# Unemployment is calculated monthly; writing Unemployment every day in the month in X\n",
    "u = Unemploymentdf.to_numpy()\n",
    "counter = r-1\n",
    "ucounter = (len(u)-1)\n",
    "for item in X:\n",
    "    if u[ucounter, 0] == Xdate[counter]:\n",
    "        ucounter = ucounter - 1\n",
    "        \n",
    "    item[2] = u[ucounter, 1]\n",
    "    counter = counter - 1\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "# Copy into column 3, the Federal Funds Rate\n",
    "# Federal Funds Rate is calculated monthly;\n",
    "f = FedFundspd.to_numpy()\n",
    "counter = r-1\n",
    "fcounter = (len(f)-1)\n",
    "for item in X:\n",
    "    if f[fcounter, 0] == Xdate[counter]:\n",
    "        fcounter = fcounter - 1\n",
    "        \n",
    "    item[3] = f[fcounter, 1]\n",
    "    counter = counter - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting data into training and testing sets using train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model RF0: Training the Random Forest Regressor, from sci-kit learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(n_estimators=1000, n_jobs=-1, random_state=1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "est = RandomForestRegressor(n_estimators=1000, criterion='mse', random_state=1, n_jobs=-1)\n",
    "est.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE train: 0.521, test: 2.904\n",
      "R^2 train: 1.000, test: 0.999\n"
     ]
    }
   ],
   "source": [
    "y_train_pred = est.predict(X_train)\n",
    "y_test_pred = est.predict(X_test)\n",
    "\n",
    "print('MSE train: %.3f, test: %.3f' % (\n",
    "        mean_squared_error(y_train, y_train_pred),\n",
    "        mean_squared_error(y_test, y_test_pred)))\n",
    "print('R^2 train: %.3f, test: %.3f' % (\n",
    "        r2_score(y_train, y_train_pred),\n",
    "        r2_score(y_test, y_test_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model RF1: Training the Random Forest Regressor with standardised data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "sc.fit(X_train)\n",
    "X_train_std = sc.transform(X_train)\n",
    "X_test_std = sc.transform(X_test)\n",
    "\n",
    "sc_y = StandardScaler()\n",
    "sc.fit(y_train[:, np.newaxis])\n",
    "y_train_std = sc.transform(y_train[:, np.newaxis]).flatten()\n",
    "y_test_std = sc.transform(y_test[:, np.newaxis]).flatten() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(n_estimators=10000, n_jobs=-1, random_state=1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "est = RandomForestRegressor(n_estimators=10000, criterion='mse', random_state=1, n_jobs=-1)\n",
    "est.fit(X_train_std, y_train_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE train: 10.117, test: 10.060\n",
      "R^2 train: -9.117, test: -9.545\n"
     ]
    }
   ],
   "source": [
    "y_train_pred = est.predict(X_train)\n",
    "y_test_pred = est.predict(X_test)\n",
    "\n",
    "print('MSE train: %.3f, test: %.3f' % (\n",
    "        mean_squared_error(y_train_std, y_train_pred),\n",
    "        mean_squared_error(y_test_std, y_test_pred)))\n",
    "print('R^2 train: %.3f, test: %.3f' % (\n",
    "        r2_score(y_train_std, y_train_pred),\n",
    "        r2_score(y_test_std, y_test_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After standardising the data, the model performs horribly. This is expected because the data that is inputted is only updated monthly or quarterly, and it is probably very difficult for the model to learn anything meaningful from this data. I asked it to predict daily fluctuations with monthly / quarterly data. From here, I will try to include sentiment analysis data of newspaper headlines in order to fine tune the result. (See SPYPredictor Sentiment Analysis.ipynb for more details)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding VADER Sentiment Analysis scores to the SPYPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publish_date</th>\n",
       "      <th>headline_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20030219</td>\n",
       "      <td>aba decides against community broadcasting lic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20030219</td>\n",
       "      <td>act fire witnesses must be aware of defamation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20030219</td>\n",
       "      <td>a g calls for infrastructure protection summit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20030219</td>\n",
       "      <td>air nz staff in aust strike for pay rise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20030219</td>\n",
       "      <td>air nz strike to affect australian travellers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1186013</th>\n",
       "      <td>20191231</td>\n",
       "      <td>vision of flames approaching corryong in victoria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1186014</th>\n",
       "      <td>20191231</td>\n",
       "      <td>wa police and government backflip on drug amne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1186015</th>\n",
       "      <td>20191231</td>\n",
       "      <td>we have fears for their safety: victorian premier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1186016</th>\n",
       "      <td>20191231</td>\n",
       "      <td>when do the 20s start</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1186017</th>\n",
       "      <td>20191231</td>\n",
       "      <td>yarraville shooting woman dead man critically ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1186018 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         publish_date                                      headline_text\n",
       "0            20030219  aba decides against community broadcasting lic...\n",
       "1            20030219     act fire witnesses must be aware of defamation\n",
       "2            20030219     a g calls for infrastructure protection summit\n",
       "3            20030219           air nz staff in aust strike for pay rise\n",
       "4            20030219      air nz strike to affect australian travellers\n",
       "...               ...                                                ...\n",
       "1186013      20191231  vision of flames approaching corryong in victoria\n",
       "1186014      20191231  wa police and government backflip on drug amne...\n",
       "1186015      20191231  we have fears for their safety: victorian premier\n",
       "1186016      20191231                              when do the 20s start\n",
       "1186017      20191231  yarraville shooting woman dead man critically ...\n",
       "\n",
       "[1186018 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer \n",
    "headlines = pd.read_csv('abcnews-date-text.csv', encoding = \"ISO-8859-1\")\n",
    "headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = headlines.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise the array with average scores as having 6151 (days) rows, and 2 columns\n",
    "r = 6151\n",
    "c = 2\n",
    "total_scores = np.empty((r, c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[20030219.        ,       -0.10707286],\n",
       "       [20030220.        ,       -0.1079212 ],\n",
       "       [20030221.        ,       -0.0993488 ],\n",
       "       ...,\n",
       "       [20191228.        ,       -0.00996333],\n",
       "       [20191229.        ,       -0.09816744],\n",
       "       [20191230.        ,       -0.17128913]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loop through large headlines dataset. Get a compound score for each headline\n",
    "# After each headline for a particular date is referenced, add it to the sum for that day\n",
    "# Once its the next day, average the scores, and add the result to total_scores\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "publish_date = a[0, 0]\n",
    "counter = 1\n",
    "score_sum = 0\n",
    "total_counter = 0\n",
    "\n",
    "for item in a:\n",
    "    if item[0] == publish_date:\n",
    "        scores = sid.polarity_scores(item[1])\n",
    "        compound_score = scores['compound']\n",
    "        score_sum = score_sum + compound_score\n",
    "        counter = counter+1\n",
    "    else:\n",
    "        total_scores[total_counter, 0] = publish_date\n",
    "        total_scores[total_counter, 1] = (score_sum/counter)\n",
    "        \n",
    "        publish_date = item[0]\n",
    "        counter = 1\n",
    "        total_counter = total_counter+1\n",
    "        score_sum = 0\n",
    "\n",
    "total_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreate X and y but within the range of the scores available for total_scores\n",
    "# Initialise the array with average scores as having 6151 (days) rows, and 6 columns\n",
    "r = 6151\n",
    "c = 5\n",
    "X_sentiment = np.empty((r, c))\n",
    "y_sentiment = np.empty((r, c))\n",
    "a = SPYdf.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Keep a separate array of the dates within total_scores\n",
    "XS_date = total_scores[:, 0]\n",
    "\n",
    "# Type for XS_date dates is wrong, so use pandas to convert it to datetime\n",
    "s = pd.DataFrame(XS_date)\n",
    "s[0] = pd.to_datetime(s[0], format='%Y%m%d') \n",
    "XS_date = s.to_numpy()\n",
    "\n",
    "# Type for Xdate dates is wrong, so use pandas to convert it to datetime\n",
    "s = pd.DataFrame(Xdate)\n",
    "s[0] = pd.to_datetime(s[0], format='%Y-%m-%d') \n",
    "Xdate = s.to_numpy()\n",
    "\n",
    "# Reference: 'How to Convert Integer to Datetime in Pandas DataFrame?', GeeksforGeeks, 2020\n",
    "# https://www.geeksforgeeks.org/how-to-convert-integer-to-datetime-in-pandas-dataframe/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through the dates, and if the dates are equal, plug in the values from the previous X and the \n",
    "# sentiment scores\n",
    "XScounter = 0\n",
    "Xcounter = 0\n",
    "\n",
    "# First get XS and X date up to the same date\n",
    "while XS_date[XScounter] != Xdate[Xcounter]:\n",
    "    Xcounter = Xcounter+1\n",
    "\n",
    "for x in range(0, 3618):\n",
    "    \n",
    "    # if XScounter < 6151 and Xcounter < 6997:\n",
    "    while XS_date[XScounter] < Xdate[Xcounter]:\n",
    "        XScounter = XScounter+1\n",
    "\n",
    "    if XS_date[XScounter] == Xdate[Xcounter]:\n",
    "        \n",
    "        X_sentiment[XScounter, 0] = X[Xcounter, 0]\n",
    "        X_sentiment[XScounter, 1] = X[Xcounter, 1]\n",
    "        X_sentiment[XScounter, 2] = X[Xcounter, 2]\n",
    "        X_sentiment[XScounter, 3] = X[Xcounter, 3]\n",
    "        X_sentiment[XScounter, 4] = total_scores[XScounter, 1]\n",
    "        y_sentiment[XScounter] = y[Xcounter]\n",
    "        \n",
    "    XScounter = XScounter+1\n",
    "    Xcounter = Xcounter+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the Random Forest Regressor, with Sentiment Analysis Data (standardised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "sc.fit(X_train)\n",
    "X_train_std = sc.transform(X_train)\n",
    "X_test_std = sc.transform(X_test)\n",
    "\n",
    "sc_y = StandardScaler()\n",
    "sc.fit(y_train[:, np.newaxis])\n",
    "y_train_std = sc.transform(y_train[:, np.newaxis]).flatten()\n",
    "y_test_std = sc.transform(y_test[:, np.newaxis]).flatten() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(n_estimators=1000, n_jobs=-1, random_state=1)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "est = RandomForestRegressor(n_estimators=1000, criterion='mse', random_state=1, n_jobs=-1)\n",
    "est.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE train: 0.521, test: 2.904\n",
      "R^2 train: 1.000, test: 0.999\n"
     ]
    }
   ],
   "source": [
    "y_train_pred = est.predict(X_train)\n",
    "y_test_pred = est.predict(X_test)\n",
    "\n",
    "print('MSE train: %.3f, test: %.3f' % (\n",
    "        mean_squared_error(y_train, y_train_pred),\n",
    "        mean_squared_error(y_test, y_test_pred)))\n",
    "print('R^2 train: %.3f, test: %.3f' % (\n",
    "        r2_score(y_train, y_train_pred),\n",
    "        r2_score(y_test, y_test_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final model with the Sentiment Analysis data performed surprisingly well. The R<sup>2</sup> of the test model was at 0.999 . This indicates that the model was able to predict the movement of SPY with the sentiment analysis data very well. However, I still question if there were flaws in my method, because these results seem almost too good. An inspection into y_test and y_test_pred, shows that each of the predictions are very close."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([137.880005, 107.529999, 125.129997, ..., 285.670013, 208.080002,\n",
       "       144.929993], dtype=object)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([137.08798356, 108.00058627, 126.09699222, ..., 287.23091665,\n",
       "       210.39007631, 146.74732295])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In conclusion, the prediction of the movement of the entire market is a very tricky process. Because the performance of the market is something that is the subject of endless debate, and the factors of the market performance is also something that is hotly contested, it is, of course, very difficult to see which factors affect the performance of SPY. However, in this project, it seems that I am able to conclude that the news and its sentiment can be an indicator for how the market moves. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the future, the sentiment analysis can be improved to yield better results. In this case, I used VADER because the presence of pre-labelled data was difficult to find, but with enough time, this data can probably be created. There is also room to grow in terms of sentiment analysis learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Works Cited: \n",
    "\n",
    "'How to Convert Integer to Datetime in Pandas DataFrame?', GeeksforGeeks, 2020\n",
    "https://www.geeksforgeeks.org/how-to-convert-integer-to-datetime-in-pandas-dataframe/\n",
    "\n",
    "\"6.2. Feature extraction\", Scikit-Learn Documentation\n",
    "https://scikit-learn.org/stable/modules/feature_extraction.html\n",
    "\n",
    "\"VADER: A Parsimonious Rule-based Model for Sentiment Analysis of Social Media Text\", by C.J. Hutto Eric Gilbert, Georgia Institute of Technology, 2014 http://eegilbert.org/papers/icwsm14.vader.hutto.pdf\n",
    "\n",
    "\"Python | Sentiment Analysis using VADER\", Geeksforgeeks.org, 2019 https://www.geeksforgeeks.org/python-sentiment-analysis-using-vader/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
